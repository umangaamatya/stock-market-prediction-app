"""
Stock Market Prediction Models
Combines technical indicators with sentiment analysis for predictions
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVR, SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report, r2_score
from sklearn.feature_selection import SelectKBest, f_regression, f_classif
import joblib
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import Config
from config.database import execute_raw_query, get_db_session

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockPredictionModel:
    """Advanced stock prediction model using multiple ML algorithms"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.label_encoders = {}
        self.feature_importance = {}
        self.performance_metrics = {}
        self.is_trained = False
        
        # Model configurations
        self.model_configs = {
            'random_forest': {
                'regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
                'classifier': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
            },
            'gradient_boosting': {
                'regressor': GradientBoostingRegressor(n_estimators=100, random_state=42),
                'classifier': None  # Will use RandomForest for classification
            },
            'linear': {
                'regressor': LinearRegression(),
                'classifier': LogisticRegression(random_state=42, max_iter=1000)
            }
        }
    
    def prepare_training_data(self, 
                            symbols: List[str] = None, 
                            days_back: int = 365,
                            include_sentiment: bool = True) -> Tuple[pd.DataFrame, Dict]:
        """
        Prepare training data by combining stock data, technical indicators, and sentiment
        
        Args:
            symbols: List of stock symbols to include
            days_back: Number of days of historical data
            include_sentiment: Whether to include sentiment features
            
        Returns:
            Tuple of (features DataFrame, metadata dictionary)
        """
        logger.info("Preparing training data")
        
        if symbols is None:
            symbols = Config.DEFAULT_STOCKS
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # Query stock data with technical indicators
        stock_query = f\"\"\"\n        SELECT s.*, t.*\n        FROM stock_data s\n        LEFT JOIN technical_indicators t ON s.symbol = t.symbol AND s.date = t.date\n        WHERE s.symbol IN ({','.join([\"'\" + symbol + \"'\" for symbol in symbols])})\n        AND s.date >= '{start_date.strftime('%Y-%m-%d')}'\n        AND s.date <= '{end_date.strftime('%Y-%m-%d')}'\n        ORDER BY s.symbol, s.date\n        \"\"\"\n        \n        stock_data = execute_raw_query(stock_query)\n        \n        if stock_data.empty:\n            logger.error("No stock data found for training")\n            return pd.DataFrame(), {}\n        \n        logger.info(f"Loaded {len(stock_data)} stock data records")\n        \n        # Add sentiment features if requested\n        if include_sentiment:\n            sentiment_data = self._get_sentiment_features(symbols, start_date, end_date)\n            if not sentiment_data.empty:\n                stock_data = self._merge_sentiment_data(stock_data, sentiment_data)\n                logger.info("Sentiment features added")\n        \n        # Create additional features\n        stock_data = self._create_advanced_features(stock_data)\n        \n        # Create target variables\n        stock_data = self._create_targets(stock_data)\n        \n        # Remove rows with missing targets (last few rows)\n        stock_data = stock_data.dropna(subset=['target_price', 'target_direction'])\n        \n        metadata = {\n            'symbols': symbols,\n            'date_range': (start_date, end_date),\n            'total_records': len(stock_data),\n            'features_count': len([col for col in stock_data.columns if col not in \n                                 ['symbol', 'date', 'target_price', 'target_direction', 'target_return']]),\n            'include_sentiment': include_sentiment\n        }\n        \n        logger.info(f"Training data prepared: {len(stock_data)} records with {metadata['features_count']} features")\n        return stock_data, metadata\n    \n    def _get_sentiment_features(self, symbols: List[str], start_date: datetime, end_date: datetime) -> pd.DataFrame:\n        """Get sentiment features from news data"""\n        sentiment_query = f\"\"\"\n        SELECT \n            symbol,\n            DATE(published_at) as date,\n            AVG(sentiment_score) as avg_sentiment,\n            COUNT(*) as news_count,\n            SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END) as positive_news,\n            SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END) as negative_news,\n            MAX(sentiment_score) as max_sentiment,\n            MIN(sentiment_score) as min_sentiment,\n            STDDEV(sentiment_score) as sentiment_volatility\n        FROM news_data\n        WHERE symbol IN ({','.join([\"'\" + symbol + \"'\" for symbol in symbols])})\n        AND DATE(published_at) >= '{start_date.strftime('%Y-%m-%d')}'\n        AND DATE(published_at) <= '{end_date.strftime('%Y-%m-%d')}'\n        GROUP BY symbol, DATE(published_at)\n        \"\"\"\n        \n        return execute_raw_query(sentiment_query)\n    \n    def _merge_sentiment_data(self, stock_data: pd.DataFrame, sentiment_data: pd.DataFrame) -> pd.DataFrame:\n        """Merge sentiment data with stock data"""\n        # Ensure date columns are datetime\n        stock_data['date'] = pd.to_datetime(stock_data['date'])\n        sentiment_data['date'] = pd.to_datetime(sentiment_data['date'])\n        \n        # Merge on symbol and date\n        merged_data = stock_data.merge(sentiment_data, on=['symbol', 'date'], how='left')\n        \n        # Fill missing sentiment values with neutral values\n        sentiment_columns = ['avg_sentiment', 'news_count', 'positive_news', 'negative_news',\n                           'max_sentiment', 'min_sentiment', 'sentiment_volatility']\n        \n        for col in sentiment_columns:\n            if col in merged_data.columns:\n                if col in ['avg_sentiment', 'max_sentiment', 'min_sentiment']:\n                    merged_data[col] = merged_data[col].fillna(0.0)\n                else:\n                    merged_data[col] = merged_data[col].fillna(0)\n        \n        return merged_data\n    \n    def _create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        """Create advanced features for better predictions"""\n        logger.info("Creating advanced features")\n        \n        # Sort by symbol and date\n        df = df.sort_values(['symbol', 'date']).reset_index(drop=True)\n        \n        # Price-based features\n        df['price_volatility_5d'] = df.groupby('symbol')['close_price'].transform(\n            lambda x: x.rolling(5).std()\n        )\n        \n        df['price_momentum_3d'] = df.groupby('symbol')['close_price'].transform(\n            lambda x: x.pct_change(3)\n        )\n        \n        df['price_acceleration'] = df.groupby('symbol')['close_price'].transform(\n            lambda x: x.pct_change().diff()\n        )\n        \n        # Volume features\n        df['volume_price_trend'] = df['volume'] * df['close_price']\n        df['volume_volatility'] = df.groupby('symbol')['volume'].transform(\n            lambda x: x.rolling(5).std()\n        )\n        \n        # Technical indicator combinations\n        df['rsi_ma_cross'] = np.where(df['rsi'] > df['rsi'].rolling(5).mean(), 1, 0)\n        df['macd_strength'] = abs(df['macd'] - df['macd_signal'])\n        df['bb_squeeze'] = (df['bb_upper'] - df['bb_lower']) / df['close_price']\n        \n        # Market context features\n        df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek\n        df['month'] = pd.to_datetime(df['date']).dt.month\n        df['quarter'] = pd.to_datetime(df['date']).dt.quarter\n        \n        # Sentiment-price interaction (if sentiment data available)\n        if 'avg_sentiment' in df.columns:\n            df['sentiment_price_interaction'] = df['avg_sentiment'] * df['price_change']\n            df['sentiment_volume_interaction'] = df['avg_sentiment'] * df['volume_ratio']\n        \n        return df\n    \n    def _create_targets(self, df: pd.DataFrame) -> pd.DataFrame:\n        """Create target variables for prediction"""\n        # Price prediction target (next day's closing price)\n        df['target_price'] = df.groupby('symbol')['close_price'].shift(-1)\n        \n        # Return prediction target\n        df['target_return'] = (df['target_price'] - df['close_price']) / df['close_price']\n        \n        # Direction prediction target (up/down/stable)\n        df['target_direction'] = np.where(df['target_return'] > 0.02, 'up',\n                                        np.where(df['target_return'] < -0.02, 'down', 'stable'))\n        \n        return df\n    \n    def train_models(self, \n                    training_data: pd.DataFrame, \n                    test_size: float = 0.2,\n                    feature_selection: bool = True,\n                    n_features: int = 50) -> Dict:\n        \"\"\"\n        Train multiple prediction models\n        \n        Args:\n            training_data: Prepared training data\n            test_size: Fraction of data for testing\n            feature_selection: Whether to perform feature selection\n            n_features: Number of top features to select\n            \n        Returns:\n            Training results dictionary\n        \"\"\"\n        logger.info("Starting model training")\n        \n        if training_data.empty:\n            logger.error("No training data provided")\n            return {}\n        \n        # Prepare features and targets\n        feature_columns = self._get_feature_columns(training_data)\n        X = training_data[feature_columns].copy()\n        y_price = training_data['target_price'].copy()\n        y_direction = training_data['target_direction'].copy()\n        \n        # Handle missing values\n        X = X.fillna(X.median())\n        \n        # Remove infinite values\n        X = X.replace([np.inf, -np.inf], np.nan)\n        X = X.fillna(X.median())\n        \n        # Split data using time series split to avoid data leakage\n        symbols = training_data['symbol'].unique()\n        train_results = {}\n        \n        for symbol in symbols:\n            logger.info(f"Training models for {symbol}")\n            \n            # Get symbol-specific data\n            symbol_mask = training_data['symbol'] == symbol\n            X_symbol = X[symbol_mask]\n            y_price_symbol = y_price[symbol_mask]\n            y_direction_symbol = y_direction[symbol_mask]\n            \n            # Remove samples with missing targets\n            valid_mask = ~(y_price_symbol.isna() | y_direction_symbol.isna())\n            X_symbol = X_symbol[valid_mask]\n            y_price_symbol = y_price_symbol[valid_mask]\n            y_direction_symbol = y_direction_symbol[valid_mask]\n            \n            if len(X_symbol) < 50:  # Need minimum samples for training\n                logger.warning(f"Not enough data for {symbol}, skipping")\n                continue\n            \n            # Time series split\n            split_idx = int(len(X_symbol) * (1 - test_size))\n            X_train = X_symbol.iloc[:split_idx]\n            X_test = X_symbol.iloc[split_idx:]\n            y_price_train = y_price_symbol.iloc[:split_idx]\n            y_price_test = y_price_symbol.iloc[split_idx:]\n            y_direction_train = y_direction_symbol.iloc[:split_idx]\n            y_direction_test = y_direction_symbol.iloc[split_idx:]\n            \n            # Scale features\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_test_scaled = scaler.transform(X_test)\n            \n            # Feature selection\n            if feature_selection:\n                # For regression\n                selector_reg = SelectKBest(score_func=f_regression, k=min(n_features, X_train.shape[1]))\n                X_train_selected_reg = selector_reg.fit_transform(X_train_scaled, y_price_train)\n                X_test_selected_reg = selector_reg.transform(X_test_scaled)\n                \n                # For classification\n                selector_clf = SelectKBest(score_func=f_classif, k=min(n_features, X_train.shape[1]))\n                label_encoder = LabelEncoder()\n                y_direction_encoded = label_encoder.fit_transform(y_direction_train)\n                X_train_selected_clf = selector_clf.fit_transform(X_train_scaled, y_direction_encoded)\n                X_test_selected_clf = selector_clf.transform(X_test_scaled)\n                \n                self.feature_selectors[f'{symbol}_reg'] = selector_reg\n                self.feature_selectors[f'{symbol}_clf'] = selector_clf\n                self.label_encoders[symbol] = label_encoder\n            else:\n                X_train_selected_reg = X_train_scaled\n                X_test_selected_reg = X_test_scaled\n                X_train_selected_clf = X_train_scaled\n                X_test_selected_clf = X_test_scaled\n            \n            self.scalers[symbol] = scaler\n            \n            symbol_results = {\n                'regression': {},\n                'classification': {}\n            }\n            \n            # Train regression models (price prediction)\n            for model_name, config in self.model_configs.items():\n                if config['regressor'] is not None:\n                    try:\n                        model = config['regressor']\n                        model.fit(X_train_selected_reg, y_price_train)\n                        \n                        # Predictions\n                        y_pred = model.predict(X_test_selected_reg)\n                        \n                        # Metrics\n                        mae = mean_absolute_error(y_price_test, y_pred)\n                        mse = mean_squared_error(y_price_test, y_pred)\n                        r2 = r2_score(y_price_test, y_pred)\n                        \n                        symbol_results['regression'][model_name] = {\n                            'model': model,\n                            'mae': mae,\n                            'mse': mse,\n                            'rmse': np.sqrt(mse),\n                            'r2': r2\n                        }\n                        \n                        # Store model\n                        self.models[f'{symbol}_{model_name}_reg'] = model\n                        \n                        logger.info(f\"{symbol} {model_name} regression - MAE: {mae:.4f}, R²: {r2:.4f}\")\n                        \n                    except Exception as e:\n                        logger.error(f"Error training {model_name} regression for {symbol}: {e}")\n            \n            # Train classification models (direction prediction)\n            for model_name, config in self.model_configs.items():\n                if config['classifier'] is not None:\n                    try:\n                        model = config['classifier']\n                        \n                        if feature_selection:\n                            y_train_encoded = label_encoder.transform(y_direction_train)\n                            y_test_encoded = label_encoder.transform(y_direction_test)\n                        else:\n                            label_encoder = LabelEncoder()\n                            y_train_encoded = label_encoder.fit_transform(y_direction_train)\n                            y_test_encoded = label_encoder.transform(y_direction_test)\n                            self.label_encoders[symbol] = label_encoder\n                        \n                        model.fit(X_train_selected_clf, y_train_encoded)\n                        \n                        # Predictions\n                        y_pred_encoded = model.predict(X_test_selected_clf)\n                        y_pred = label_encoder.inverse_transform(y_pred_encoded)\n                        \n                        # Metrics\n                        accuracy = accuracy_score(y_direction_test, y_pred)\n                        \n                        symbol_results['classification'][model_name] = {\n                            'model': model,\n                            'accuracy': accuracy\n                        }\n                        \n                        # Store model\n                        self.models[f'{symbol}_{model_name}_clf'] = model\n                        \n                        logger.info(f\"{symbol} {model_name} classification - Accuracy: {accuracy:.4f}\")\n                        \n                    except Exception as e:\n                        logger.error(f\"Error training {model_name} classification for {symbol}: {e}\")\n            \n            train_results[symbol] = symbol_results\n        \n        self.performance_metrics = train_results\n        self.is_trained = True\n        \n        logger.info("Model training completed")\n        return train_results\n    \n    def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Get list of feature columns (excluding targets and metadata)\"\"\"\n        exclude_columns = [\n            'symbol', 'date', 'target_price', 'target_direction', 'target_return',\n            'created_at', 'updated_at', 'id'\n        ]\n        \n        feature_columns = [col for col in df.columns if col not in exclude_columns]\n        return feature_columns\n    \n    def predict(self, \n               symbol: str, \n               features: pd.DataFrame, \n               model_type: str = 'random_forest') -> Dict:\n        \"\"\"\n        Make predictions for a symbol\n        \n        Args:\n            symbol: Stock symbol\n            features: Feature data for prediction\n            model_type: Type of model to use\n            \n        Returns:\n            Dictionary with predictions\n        \"\"\"\n        if not self.is_trained:\n            logger.error("Models not trained. Call train_models() first.")\n            return {}\n        \n        # Get models for the symbol\n        reg_model_key = f'{symbol}_{model_type}_reg'\n        clf_model_key = f'{symbol}_{model_type}_clf'\n        \n        if reg_model_key not in self.models or clf_model_key not in self.models:\n            logger.error(f"No trained models found for {symbol} with {model_type}")\n            return {}\n        \n        try:\n            # Prepare features\n            feature_columns = self._get_feature_columns(features)\n            X = features[feature_columns].copy()\n            X = X.fillna(X.median())\n            X = X.replace([np.inf, -np.inf], np.nan)\n            X = X.fillna(X.median())\n            \n            # Scale features\n            scaler = self.scalers.get(symbol)\n            if scaler is None:\n                logger.error(f"No scaler found for {symbol}")\n                return {}\n            \n            X_scaled = scaler.transform(X)\n            \n            # Apply feature selection\n            if f'{symbol}_reg' in self.feature_selectors:\n                X_reg = self.feature_selectors[f'{symbol}_reg'].transform(X_scaled)\n                X_clf = self.feature_selectors[f'{symbol}_clf'].transform(X_scaled)\n            else:\n                X_reg = X_scaled\n                X_clf = X_scaled\n            \n            # Price prediction\n            reg_model = self.models[reg_model_key]\n            predicted_price = reg_model.predict(X_reg)[0]\n            \n            # Direction prediction\n            clf_model = self.models[clf_model_key]\n            predicted_direction_encoded = clf_model.predict(X_clf)[0]\n            label_encoder = self.label_encoders[symbol]\n            predicted_direction = label_encoder.inverse_transform([predicted_direction_encoded])[0]\n            \n            # Confidence scores\n            if hasattr(clf_model, 'predict_proba'):\n                confidence_scores = clf_model.predict_proba(X_clf)[0]\n                confidence = np.max(confidence_scores)\n            else:\n                confidence = 0.5\n            \n            current_price = features['close_price'].iloc[0] if 'close_price' in features.columns else 0\n            predicted_return = (predicted_price - current_price) / current_price if current_price > 0 else 0\n            \n            return {\n                'symbol': symbol,\n                'predicted_price': predicted_price,\n                'current_price': current_price,\n                'predicted_return': predicted_return,\n                'predicted_direction': predicted_direction,\n                'confidence': confidence,\n                'model_type': model_type,\n                'prediction_date': datetime.now()\n            }\n            \n        except Exception as e:\n            logger.error(f"Error making prediction for {symbol}: {e}")\n            return {}\n    \n    def save_models(self, filepath: str = None):\n        \"\"\"Save trained models to disk\"\"\"\n        if not self.is_trained:\n            logger.warning("No trained models to save")\n            return\n        \n        if filepath is None:\n            filepath = os.path.join(Config.MODEL_DIR, f'stock_prediction_models_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n        \n        # Ensure directory exists\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        \n        model_data = {\n            'models': self.models,\n            'scalers': self.scalers,\n            'feature_selectors': self.feature_selectors,\n            'label_encoders': self.label_encoders,\n            'performance_metrics': self.performance_metrics,\n            'model_configs': self.model_configs,\n            'is_trained': self.is_trained,\n            'saved_at': datetime.now()\n        }\n        \n        joblib.dump(model_data, filepath)\n        logger.info(f"Models saved to {filepath}")\n    \n    def load_models(self, filepath: str):\n        \"\"\"Load trained models from disk\"\"\"\n        try:\n            model_data = joblib.load(filepath)\n            \n            self.models = model_data['models']\n            self.scalers = model_data['scalers']\n            self.feature_selectors = model_data['feature_selectors']\n            self.label_encoders = model_data['label_encoders']\n            self.performance_metrics = model_data['performance_metrics']\n            self.model_configs = model_data.get('model_configs', self.model_configs)\n            self.is_trained = model_data['is_trained']\n            \n            logger.info(f"Models loaded from {filepath}")\n            logger.info(f"Saved at: {model_data.get('saved_at', 'Unknown')}\")\n            \n        except Exception as e:\n            logger.error(f"Error loading models: {e}")\n    \n    def get_model_performance(self) -> Dict:\n        \"\"\"Get performance metrics for all trained models\"\"\"\n        return self.performance_metrics\n    \n    def get_feature_importance(self, symbol: str, model_type: str = 'random_forest') -> Dict:\n        \"\"\"Get feature importance for a specific model\"\"\"\n        model_key = f'{symbol}_{model_type}_reg'\n        if model_key not in self.models:\n            return {}\n        \n        model = self.models[model_key]\n        if hasattr(model, 'feature_importances_'):\n            # Get feature names (this is simplified - in practice, you'd want to track the exact feature names after selection)\n            importances = model.feature_importances_\n            return {\n                'model': f'{symbol}_{model_type}',\n                'importance_values': importances.tolist(),\n                'feature_count': len(importances)\n            }\n        \n        return {}
